"""
UIFace-Plus: æ‰©æ•£æ¨¡å‹äººè„¸ç”Ÿæˆå¯è§†åŒ–ç³»ç»Ÿ

æœ¬è„šæœ¬å®ç°äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„äººè„¸ç”Ÿæˆï¼Œå¹¶å¯è§†åŒ–å»å™ªè¿‡ç¨‹ã€‚
ä¸»è¦åŠŸèƒ½ï¼š
1. ä½¿ç”¨ DDIM é‡‡æ ·ç”Ÿæˆé«˜è´¨é‡äººè„¸
2. é›†æˆ Classifier-Free Guidance (CFG) æå‡ç”Ÿæˆè´¨é‡
3. å¯è§†åŒ–æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼ˆä»å™ªå£°åˆ°æ¸…æ™°äººè„¸ï¼‰

ä½œè€…ï¼šå¤æ—¦å¤§å­¦ ç”Ÿæˆæ¨¡å‹è¯¾ç¨‹
æ—¥æœŸï¼š2024å¹´12æœˆ
"""

import os
import sys
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import yaml
from pathlib import Path

generation_dir = "E:/FDU/è¯¾ç¨‹/ç”Ÿæˆæ¨¡å‹/TFace-master/generation"
uiface_dir = os.path.join(generation_dir, "uiface")
sys.path.insert(0, uiface_dir)

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
from models.diffusion.unet import ConditionalUNet
from models.autoencoder.vqgan import VQEncoderInterface, VQDecoderInterface
from diffusion.ddpm import DenoisingDiffusionProbabilisticModel


class UIFacePlusGenerator:
    """å¢å¼ºç‰ˆ UIFace ç”Ÿæˆå™¨ï¼Œæ”¯æŒå¯è§†åŒ–æ‰©æ•£è¿‡ç¨‹"""

    def __init__(self, config_path, checkpoint_path, vq_encoder_path, vq_decoder_path, device='cuda'):
        self.device = device

        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        # åŠ è½½æ‰©æ•£æ¨¡å‹
        print("Loading diffusion model...")
        self.unet = self._load_unet(checkpoint_path)
        self.unet.to(device)
        self.unet.eval()

        # åŠ è½½è‡ªåŠ¨ç¼–ç å™¨
        print("Loading autoencoder...")
        self.vq_encoder = self._load_autoencoder(vq_encoder_path, encoder=True)
        self.vq_decoder = self._load_autoencoder(vq_decoder_path, encoder=False)

        # åˆ›å»ºæ‰©æ•£è¿‡ç¨‹
        self.diffusion = DenoisingDiffusionProbabilisticModel(
            eps_model=self.unet,
            T=1000,
            schedule_type='linear',
            schedule_beta_min=0.0001,
            schedule_beta_max=0.02
        )

        # åŠ¨æ€è®¡ç®— latent ç©ºé—´å°ºå¯¸ï¼ˆå‚è€ƒå®˜æ–¹ sample.py:56,65ï¼‰
        image_size = (3, 128, 128)  # UIFace è®­ç»ƒæ—¶ä½¿ç”¨çš„å›¾åƒå°ºå¯¸
        with torch.no_grad():
            dummy_input = torch.ones([1, *image_size]).to(device)
            self.latent_shape = self.vq_encoder(dummy_input).shape[1:]  # (C, H, W)
        print(f"Latent shape: {self.latent_shape}")

        print("Model loaded successfully!")

    def _load_unet(self, checkpoint_path):
        """åŠ è½½ UNet æ¨¡å‹"""
        # ä½¿ç”¨ UIFace çš„é…ç½®å‚æ•°
        unet = ConditionalUNet(
            input_channels=3,
            initial_channels=96,
            channel_multipliers=(1, 2, 2, 2),
            is_attention=(False, True, True, True),
            attention_heads=-1,
            attention_head_channels=32,
            n_blocks_per_resolution=2,
            condition_type="CA",
            is_context_conditional=True,
            n_context_classes=0,
            context_input_channels=512,
            context_channels=256,
            learn_empty_context=True,
            context_dropout_probability=0.25,
            unconditioned_probability=0.2,
        )

        # åŠ è½½æƒé‡
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location='cpu')

            # æå–å®é™…çš„ state_dict
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint

            # å»æ‰ 'module.eps_model.' å‰ç¼€ï¼Œå¹¶è¿‡æ»¤æ‰ diffusion ç›¸å…³çš„å‚æ•°
            diffusion_keys = {'betas', 'alphas', 'sigmas', 'sqrt_alphas_inv', 'alpha_bars',
                             'sqrt_alpha_bars', 'sqrt_one_minus_alpha_bars',
                             'one_minus_alphas_over_sqrt_one_minus_alpha_bars',
                             'alphas_prev', 'alphas_next'}

            new_state_dict = {}
            for key, value in state_dict.items():
                # è·³è¿‡ diffusion ç›¸å…³çš„å‚æ•°
                if any(key.endswith(dk) or key == f'module.{dk}' for dk in diffusion_keys):
                    continue

                if key.startswith('module.eps_model.'):
                    new_key = key.replace('module.eps_model.', '')
                    new_state_dict[new_key] = value
                elif key.startswith('module.'):
                    new_key = key.replace('module.', '')
                    new_state_dict[new_key] = value
                else:
                    new_state_dict[key] = value

            unet.load_state_dict(new_state_dict, strict=False)
            print(f"Loaded checkpoint from {checkpoint_path}")

        return unet

    def _load_autoencoder(self, path, encoder=True):
        """åŠ è½½è‡ªåŠ¨ç¼–ç å™¨"""
        if not os.path.exists(path):
            return None

        # VQ-VAE éœ€è¦é…ç½®æ–‡ä»¶è·¯å¾„
        config_path = os.path.join(uiface_dir, 'models', 'autoencoder', 'first_stage_config.yaml')

        if encoder:
            model = VQEncoderInterface(config_path, path)
        else:
            model = VQDecoderInterface(config_path, path)

        model.to(self.device)
        model.eval()
        return model

    def _get_betas(self):
        """è·å–æ‰©æ•£è¿‡ç¨‹çš„ beta schedule"""
        num_steps = self.config.get('diffusion', {}).get('num_diffusion_timesteps', 1000)
        beta_schedule = self.config.get('diffusion', {}).get('beta_schedule', 'linear')

        if beta_schedule == 'linear':
            beta_start = 0.0001
            beta_end = 0.02
            return np.linspace(beta_start, beta_end, num_steps, dtype=np.float32)
        else:
            raise NotImplementedError(f"Beta schedule {beta_schedule} not implemented")

    @torch.no_grad()
    def generate_with_steps(self, identity_embedding=None, num_steps=50, save_intermediate=True, cfg_scale=1.5):
        """
        ç”Ÿæˆäººè„¸å¹¶ä¿å­˜ä¸­é—´æ­¥éª¤

        Args:
            identity_embedding: èº«ä»½åµŒå…¥å‘é‡ï¼Œå¦‚æœä¸º None åˆ™éšæœºç”Ÿæˆ
            num_steps: DDIM é‡‡æ ·æ­¥æ•°
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´æ­¥éª¤
            cfg_scale: Classifier-Free Guidance å¼ºåº¦ï¼ˆå‚è€ƒå®˜æ–¹ sample.pyï¼‰

        Returns:
            final_image: æœ€ç»ˆç”Ÿæˆçš„å›¾åƒ
            intermediate_images: ä¸­é—´æ­¥éª¤çš„å›¾åƒåˆ—è¡¨
        """
        # åˆå§‹åŒ–å™ªå£°ï¼ˆåœ¨ latent ç©ºé—´ï¼‰
        latent_shape = (1, *self.latent_shape)  # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„ latent å°ºå¯¸
        x_t = torch.randn(latent_shape, device=self.device)

        # å¦‚æœæ²¡æœ‰æä¾›èº«ä»½åµŒå…¥ï¼Œä½¿ç”¨éšæœºåµŒå…¥å¹¶å½’ä¸€åŒ–ï¼ˆå‚è€ƒå®˜æ–¹ sample.py:85-86ï¼‰
        if identity_embedding is None:
            identity_embedding = np.random.randn(512)
            identity_embedding = identity_embedding / np.linalg.norm(identity_embedding)
            identity_embedding = torch.tensor(identity_embedding, dtype=torch.float32, device=self.device).unsqueeze(0)
        else:
            identity_embedding = torch.tensor(identity_embedding, device=self.device).unsqueeze(0)

        # é‡‡æ ·æ­¥éª¤ï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:82ï¼‰
        skip = self.diffusion.T // num_steps

        intermediate_images = []
        intermediate_latents = []

        print(f"Generating face with {num_steps} denoising steps (skip={skip}, CFG scale={cfg_scale})...")

        # DDIM é‡‡æ ·å¾ªç¯ï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:87-115ï¼‰
        for i in reversed(range(0, self.diffusion.T, skip)):
            t_batch = torch.tensor([i], device=self.device)

            # CFG: é¢„æµ‹æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„å™ªå£°ï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:228-244ï¼‰
            noise_pred_uncond, _, _ = self.unet(x_t, t_batch, None)  # æ— æ¡ä»¶
            noise_pred_cond, _, _ = self.unet(x_t, t_batch, identity_embedding)  # æœ‰æ¡ä»¶

            # CFG ç»„åˆï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:242-244ï¼‰
            noise_pred = (1 + cfg_scale) * noise_pred_cond - cfg_scale * noise_pred_uncond

            # DDIM æ›´æ–°ï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:94-115ï¼‰
            prev_timestep = i - skip
            alpha_prod_t = self.diffusion.alpha_bars[i]
            alpha_prod_t_prev = (
                self.diffusion.alphas_prev[prev_timestep]
                if prev_timestep >= 0
                else torch.tensor(1.0, device=self.device)
            )
            beta_prod_t = 1 - alpha_prod_t

            # é¢„æµ‹ x0ï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:101-102ï¼‰
            pred_x0 = (x_t - torch.sqrt(beta_prod_t) * noise_pred) / torch.sqrt(alpha_prod_t)

            # è®¡ç®— x_{t-1}ï¼ˆå‚è€ƒå®˜æ–¹ ddpm.py:106-109ï¼Œeta=0çš„DDIMï¼‰
            pred_x0_direction = torch.sqrt(alpha_prod_t_prev) * pred_x0
            pred_noise_direction = torch.sqrt(1 - alpha_prod_t_prev) * noise_pred
            x_t = pred_x0_direction + pred_noise_direction

            # ä¿å­˜ä¸­é—´æ­¥éª¤
            if save_intermediate and (i % (self.diffusion.T // 10) == 0 or i == 0):
                intermediate_latents.append(x_t.cpu().clone())

        # è§£ç æœ€ç»ˆæ½œåœ¨è¡¨ç¤º
        if self.vq_decoder is not None:
            final_image = self.vq_decoder(x_t)  # forward æ–¹æ³•å°±æ˜¯è§£ç 
            final_image = self._tensor_to_image(final_image)

            # è§£ç ä¸­é—´æ­¥éª¤
            for latent in intermediate_latents:
                img = self.vq_decoder(latent.to(self.device))
                intermediate_images.append(self._tensor_to_image(img))
        else:
            final_image = self._tensor_to_image(x_t)
            intermediate_images = [self._tensor_to_image(lat.to(self.device)) for lat in intermediate_latents]

        return final_image, intermediate_images

    def _tensor_to_image(self, tensor):
        """å°† tensor è½¬æ¢ä¸º PIL Image"""
        # åå½’ä¸€åŒ–
        img = tensor.squeeze(0).cpu().numpy()
        img = np.transpose(img, (1, 2, 0))
        img = (img + 1) / 2  # [-1, 1] -> [0, 1]
        img = np.clip(img * 255, 0, 255).astype(np.uint8)
        return Image.fromarray(img)

    def visualize_denoising_process(self, intermediate_images, save_path):
        """å¯è§†åŒ–å»å™ªè¿‡ç¨‹"""
        num_images = len(intermediate_images)
        fig, axes = plt.subplots(2, (num_images + 1) // 2, figsize=(15, 6))
        axes = axes.flatten()

        for i, img in enumerate(intermediate_images):
            axes[i].imshow(img)
            axes[i].set_title(f'Step {i * (100 // num_images)}%')
            axes[i].axis('off')

        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_images, len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"Denoising visualization saved to {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„ï¼ˆéœ€è¦ç”¨æˆ·å¡«å†™ï¼‰
    config = {
        'model_config': './generation/uiface/configs/model/unet_cond_ca_cpd25_uncond20.yaml',
        'checkpoint': 'UIFace-Plus\models\\ema_averaged_model_250000.ckpt',  # éœ€è¦ä¸‹è½½
        'vq_encoder': 'UIFace-Plus\models\\first_stage_encoder_state_dict.pt',  # éœ€è¦ä¸‹è½½
        'vq_decoder': 'UIFace-Plus\models\\first_stage_decoder_state_dict.pt',  # éœ€è¦ä¸‹è½½
    }

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for key, path in config.items():
        if not os.path.exists(path) and 'path/to' not in path:
            print(f"Warning: {key} not found at {path}")

    # åˆ›å»ºç”Ÿæˆå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    try:
        generator = UIFacePlusGenerator(
            config_path=config['model_config'],
            checkpoint_path=config['checkpoint'],
            vq_encoder_path=config['vq_encoder'],
            vq_decoder_path=config['vq_decoder'],
            device=device
        )

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path('outputs/generation')
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆå¤šä¸ªäººè„¸
        num_faces = 5
        for i in range(num_faces):
            print(f"\n=== Generating face {i+1}/{num_faces} ===")

            # ç”Ÿæˆäººè„¸ï¼ˆä½¿ç”¨å®˜æ–¹æ¨èçš„50æ­¥DDIMé‡‡æ ·ï¼‰
            final_image, intermediate_images = generator.generate_with_steps(
                num_steps=100,
                save_intermediate=True,
                cfg_scale=1.5  # CFGå¼•å¯¼å¼ºåº¦ï¼Œå®˜æ–¹é»˜è®¤å€¼
            )

            # ä¿å­˜æœ€ç»ˆå›¾åƒ
            final_image.save(output_dir / f'face_{i+1}_final.png')
            print(f"Saved final image to {output_dir / f'face_{i+1}_final.png'}")

            # å¯è§†åŒ–å»å™ªè¿‡ç¨‹
            if intermediate_images:
                generator.visualize_denoising_process(
                    intermediate_images,
                    output_dir / f'face_{i+1}_denoising_process.png'
                )

        print("\nâœ… Generation completed successfully!")
        print(f"ğŸ“ Check outputs in: {output_dir.absolute()}")

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("\nğŸ’¡ æç¤º: è¯·å…ˆä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡:")
        print("   1. UIFace æ‰©æ•£æ¨¡å‹: https://drive.google.com/drive/folders/11OnYj0mtEkepjl3gE2oLeDJu_WeuB0Ma")
        print("   2. VQ-VAE ç¼–ç å™¨/è§£ç å™¨: https://drive.google.com/drive/folders/1d-zs3yjsnzOMNkz7qy3JSb-fMf0UmSdT")
        print("\n   ç„¶åæ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„")


if __name__ == '__main__':
    main()
